<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Build Your Own AI Video Generation with LTX-Video - Complete Tutorial</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 15px 50px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 50px 20px;
            text-align: center;
        }

        header h1 {
            font-size: 2.8em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        header p {
            font-size: 1.2em;
            opacity: 0.95;
        }

        .tech-badge {
            display: inline-block;
            background: rgba(255, 255, 255, 0.2);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            margin: 10px 5px;
            font-size: 0.9em;
            border: 1px solid rgba(255, 255, 255, 0.3);
        }

        .attribution {
            background: #f0f7ff;
            padding: 15px;
            margin: 20px;
            border-left: 4px solid #2a5298;
            border-radius: 4px;
            font-size: 0.95em;
        }

        .attribution a {
            color: #2a5298;
            text-decoration: none;
            font-weight: bold;
        }

        .attribution a:hover {
            text-decoration: underline;
        }

        main {
            padding: 40px;
        }

        section {
            margin-bottom: 45px;
        }

        h2 {
            color: #1e3c72;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #2a5298;
            padding-bottom: 10px;
        }

        h3 {
            color: #2a5298;
            font-size: 1.6em;
            margin-top: 28px;
            margin-bottom: 15px;
        }

        h4 {
            color: #1e3c72;
            font-size: 1.2em;
            margin-top: 20px;
            margin-bottom: 12px;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
            line-height: 1.8;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 12px;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d63384;
        }

        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 4px solid #2a5298;
        }

        .code-block code {
            background: none;
            color: #f8f8f2;
            padding: 0;
        }

        .feature-box {
            background: #f9f9f9;
            border: 2px solid #2a5298;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .feature-box h4 {
            color: #2a5298;
            margin-top: 0;
        }

        .feature-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        @media (max-width: 768px) {
            .feature-grid {
                grid-template-columns: 1fr;
            }
        }

        .step-box {
            background: #e8f4f8;
            border-left: 4px solid #2a5298;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .step-box strong {
            color: #1e3c72;
        }

        .highlight {
            background: #fffbea;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #f39c12;
            margin: 20px 0;
        }

        .key-takeaway {
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .warning {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .tip {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        .resources {
            background: #f0f7ff;
            padding: 25px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .resources a {
            color: #2a5298;
            text-decoration: none;
            font-weight: bold;
        }

        .resources a:hover {
            text-decoration: underline;
        }

        .model-comparison {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .model-comparison th, .model-comparison td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }

        .model-comparison th {
            background: #2a5298;
            color: white;
        }

        .model-comparison tr:nth-child(even) {
            background: #f9f9f9;
        }

        .diagram-placeholder {
            background: #e8e8e8;
            border: 2px dashed #999;
            padding: 40px;
            text-align: center;
            margin: 20px 0;
            border-radius: 8px;
            color: #666;
        }

        footer {
            background: #f5f5f5;
            padding: 30px;
            text-align: center;
            color: #666;
            font-size: 0.95em;
            border-top: 1px solid #ddd;
        }

        footer a {
            color: #2a5298;
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        .emoji {
            margin-right: 5px;
        }

        .command-prompt {
            background: #1e1e1e;
            color: #4ec9b0;
            padding: 15px;
            border-radius: 4px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            border-left: 3px solid #2a5298;
        }

        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }

        a {
            color: #2a5298;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc {
            background: #f5f5f5;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #2a5298;
        }

        .toc h3 {
            margin-top: 0;
        }

        .toc ul {
            margin-bottom: 0;
        }

        .toc a {
            text-decoration: none;
        }

        .toc a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üé¨ Build Your Own AI Video Generation</h1>
            <p>Complete Tutorial: LTX-Video by Lightricks</p>
            <div>
                <span class="tech-badge">ü§ñ AI Video</span>
                <span class="tech-badge">üé• Text-to-Video</span>
                <span class="tech-badge">üñºÔ∏è Image-to-Video</span>
                <span class="tech-badge">üíª Open Source</span>
            </div>
        </header>

        <div class="attribution">
            <strong>üìö Based On:</strong> <a href="https://github.com/Lightricks/LTX-Video" target="_blank">
            LTX-Video Official Repository by Lightricks</a>
            <br>
            <strong>License:</strong> <a href="https://github.com/Lightricks/LTX-Video/blob/main/LICENSE" target="_blank">Apache 2.0</a>
            <br>
            <strong>Paper:</strong> <a href="https://arxiv.org/abs/2501.00103" target="_blank">"LTX-Video: Realtime Video Latent Diffusion"</a>
        </div>

        <main>
            <!-- Table of Contents -->
            <div class="toc">
                <h3>üìñ Table of Contents</h3>
                <ul>
                    <li><a href="#intro">Introduction to LTX-Video</a></li>
                    <li><a href="#features">Core Features & Capabilities</a></li>
                    <li><a href="#models">Available Models</a></li>
                    <li><a href="#setup">Installation & Setup</a></li>
                    <li><a href="#getting-started">Getting Started</a></li>
                    <li><a href="#guide">Usage Guide</a></li>
                    <li><a href="#advanced">Advanced Features</a></li>
                    <li><a href="#tips">Pro Tips & Optimization</a></li>
                    <li><a href="#resources">Resources & Further Learning</a></li>
                </ul>
            </div>

            <!-- Introduction -->
            <section id="intro">
                <h2>üöÄ Introduction to LTX-Video</h2>
                <p>
                    LTX-Video is a groundbreaking DiT-based (Diffusion Transformer) video generation model created by Lightricks. 
                    It's the first model that combines all core capabilities of modern video generation into a single, efficient system.
                </p>

                <p>
                    Unlike previous video generation models that require separate components for different tasks, 
                    LTX-Video is an all-in-one solution that can:
                </p>

                <ul>
                    <li><strong>Generate 4K videos</strong> with native 4K resolution and up to 50 FPS</li>
                    <li><strong>Work in real-time</strong> on consumer hardware</li>
                    <li><strong>Create videos from text</strong> or images</li>
                    <li><strong>Extend existing videos</strong> forward or backward</li>
                    <li><strong>Apply advanced transformations</strong> with precise control</li>
                    <li><strong>Support multiple conditioning modes</strong> simultaneously</li>
                </ul>

                <div class="key-takeaway">
                    <strong>‚úÖ Key Advantage:</strong> LTX-Video is open-source and production-ready, meaning you can run it 
                    locally or deploy it on your own infrastructure without vendor lock-in.
                </div>

                <h3>What Makes LTX-Video Special?</h3>
                <ul>
                    <li><strong>Efficiency:</strong> Up to 50% lower compute cost than competing models</li>
                    <li><strong>Quality:</strong> Production-grade outputs with realistic and diverse content</li>
                    <li><strong>Speed:</strong> Real-time generation on high-end GPUs, fast iteration on consumer GPUs</li>
                    <li><strong>Flexibility:</strong> Supports text-to-video, image-to-video, video extension, and combinations</li>
                    <li><strong>Control:</strong> Multiple keyframe conditioning and precise creative control via LoRA</li>
                    <li><strong>Accessibility:</strong> Apache 2.0 licensed, fully open-source</li>
                </ul>
            </section>

            <!-- Features -->
            <section id="features">
                <h2>‚ú® Core Features & Capabilities</h2>

                <h3>1. Text-to-Video Generation</h3>
                <p>
                    Generate videos from detailed text descriptions. The model understands complex prompts describing:
                </p>
                <ul>
                    <li>Character movements and gestures</li>
                    <li>Scene composition and environments</li>
                    <li>Camera angles and movements</li>
                    <li>Lighting and color palettes</li>
                    <li>Dynamic changes and events</li>
                </ul>

                <h3>2. Image-to-Video Generation</h3>
                <p>
                    Bring static images to life by generating video sequences that animate them. Perfect for:
                </p>
                <ul>
                    <li>Animating photographs</li>
                    <li>Creating cinemagraphs</li>
                    <li>Extending static art with motion</li>
                    <li>Transforming designs into videos</li>
                </ul>

                <h3>3. Multi-Keyframe Conditioning</h3>
                <p>
                    Control video generation with multiple keyframes at different timestamps, enabling:
                </p>
                <ul>
                    <li>Frame-level precision for specific scenes</li>
                    <li>Smooth transitions between states</li>
                    <li>Complex multi-scene video creation</li>
                    <li>Consistent character/object positioning</li>
                </ul>

                <h3>4. Video Extension (Forward & Backward)</h3>
                <p>
                    Extend existing videos in either direction to create longer, continuous sequences without restarting generation.
                </p>

                <h3>5. Video-to-Video Transformation</h3>
                <p>
                    Modify existing videos with text prompts to apply effects, change styles, or alter content while preserving motion.
                </p>

                <h3>6. Control Models (IC-LoRA)</h3>
                <p>
                    Use specialized control models for precise guidance:
                </p>
                <ul>
                    <li><strong>Depth Control:</strong> Guide composition and 3D structure</li>
                    <li><strong>Pose Control:</strong> Control character/object positioning</li>
                    <li><strong>Canny Control:</strong> Precise edge-based control</li>
                </ul>

                <div class="feature-grid">
                    <div class="feature-box">
                        <h4>‚ö° Performance Tiers</h4>
                        <p><strong>Full Models (13B):</strong> Maximum quality</p>
                        <p><strong>Distilled Models:</strong> 15√ó faster, 1.5GB+ VRAM</p>
                        <p><strong>2B Models:</strong> Fastest, minimal VRAM</p>
                    </div>
                    <div class="feature-box">
                        <h4>üìä Resolution Support</h4>
                        <p>Works on any resolution divisible by 32</p>
                        <p>Optimal: Up to 720√ó1280</p>
                        <p>Maximum: 4K native resolution</p>
                    </div>
                </div>
            </section>

            <!-- Models -->
            <section id="models">
                <h2>ü§ñ Available Models</h2>

                <h3>Model Comparison</h3>
                <table class="model-comparison">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Size</th>
                            <th>Quality</th>
                            <th>Speed</th>
                            <th>VRAM</th>
                            <th>Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>ltxv-13b-0.9.8-dev</strong></td>
                            <td>13B</td>
                            <td>Highest</td>
                            <td>Slower</td>
                            <td>~24GB</td>
                            <td>Production quality</td>
                        </tr>
                        <tr>
                            <td><strong>ltxv-13b-0.9.8-distilled</strong></td>
                            <td>13B</td>
                            <td>High</td>
                            <td>15√ó faster</td>
                            <td>~8GB</td>
                            <td>Fast iteration</td>
                        </tr>
                        <tr>
                            <td><strong>ltxv-13b-0.9.8-dev-fp8</strong></td>
                            <td>13B</td>
                            <td>Highest</td>
                            <td>Very Fast</td>
                            <td>~12GB</td>
                            <td>Quantized high quality</td>
                        </tr>
                        <tr>
                            <td><strong>ltxv-13b-0.9.8-distilled-fp8</strong></td>
                            <td>13B</td>
                            <td>High</td>
                            <td>Fastest</td>
                            <td>~6GB</td>
                            <td>Real-time generation</td>
                        </tr>
                        <tr>
                            <td><strong>ltxv-2b-0.9.8-distilled</strong></td>
                            <td>2B</td>
                            <td>Good</td>
                            <td>Very Fast</td>
                            <td>~3GB</td>
                            <td>Consumer GPUs</td>
                        </tr>
                        <tr>
                            <td><strong>ltxv-2b-0.9.8-distilled-fp8</strong></td>
                            <td>2B</td>
                            <td>Good</td>
                            <td>Fastest</td>
                            <td>~2GB</td>
                            <td>Minimal hardware</td>
                        </tr>
                    </tbody>
                </table>

                <div class="tip">
                    <strong>üí° Model Selection Guide:</strong>
                    <ul>
                        <li>If you have H100/A100 GPU: Use full 13B model for best quality</li>
                        <li>If you have RTX 4090 or RTX 4080: Use 13B-distilled for balanced results</li>
                        <li>If you have RTX 4070/4080 with limited VRAM: Use 2B-distilled</li>
                        <li>If you have RTX 4060 or less: Use 2B-distilled-fp8</li>
                    </ul>
                </div>
            </section>

            <!-- Setup & Installation -->
            <section id="setup">
                <h2>‚öôÔ∏è Installation & Setup Guide</h2>

                <h3>Prerequisites</h3>
                <ul>
                    <li>Python 3.10+ (tested with 3.10.5)</li>
                    <li>CUDA 12.2+ (for NVIDIA GPUs) or MPS (for macOS)</li>
                    <li>PyTorch 2.1.2+</li>
                    <li>Git</li>
                    <li>A GPU with adequate VRAM (see model comparison above)</li>
                </ul>

                <h3>Step 1: Clone the Repository</h3>
                <div class="command-prompt">
                    git clone https://github.com/Lightricks/LTX-Video.git<br>
                    cd LTX-Video
                </div>

                <h3>Step 2: Create Virtual Environment</h3>
                <div class="command-prompt">
                    python -m venv env<br>
                    source env/bin/activate  # On Windows: env\Scripts\activate
                </div>

                <h3>Step 3: Install Dependencies</h3>
                <div class="command-prompt">
                    python -m pip install -e .[inference]
                </div>

                <div class="success">
                    <strong>‚úÖ Installation Complete!</strong> You now have LTX-Video ready to use.
                </div>

                <h3>Optional: Install FP8 Kernels (For Speedup)</h3>
                <p>
                    If you have an NVIDIA GPU with Ada architecture (RTX 40 series) or newer, 
                    you can install FP8 kernels for significant performance improvements:
                </p>
                <div class="command-prompt">
                    git clone https://github.com/Lightricks/LTXVideo-Q8-Kernels.git<br>
                    cd LTXVideo-Q8-Kernels<br>
                    python setup.py install
                </div>

                <div class="warning">
                    <strong>‚ö†Ô∏è Note:</strong> FP8 kernels are optional and only work on Ada-architecture GPUs or newer.
                </div>

                <h3>Download Models</h3>
                <p>
                    Models are automatically downloaded from Hugging Face when first used. 
                    However, you can pre-download them:
                </p>
                <div class="command-prompt">
                    # Using Hugging Face CLI<br>
                    pip install huggingface-hub<br>
                    huggingface-cli download Lightricks/LTX-Video --repo-type model
                </div>

                <div class="tip">
                    <strong>üí° Tip:</strong> Set <code>HF_HOME</code> environment variable to control where models are cached:
                    <div class="command-prompt">
                        export HF_HOME=/path/to/models  # or set HF_HOME on Windows
                    </div>
                </div>
            </section>

            <!-- Getting Started -->
            <section id="getting-started">
                <h2>üé¨ Getting Started: Your First Video</h2>

                <h3>Generate a Video from Text</h3>
                <p>Let's create your first AI-generated video:</p>

                <div class="code-block">
<code>python inference.py \
  --prompt "A red fox running through a snow-covered forest, with trees passing by at high speed, golden sunset light filtering through the trees" \
  --height 720 \
  --width 1280 \
  --num_frames 121 \
  --seed 42 \
  --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml</code>
                </div>

                <div class="step-box">
                    <strong>What this does:</strong>
                    <ul>
                        <li><code>--prompt</code>: Your text description of what you want to see</li>
                        <li><code>--height/--width</code>: Video resolution (must be divisible by 32)</li>
                        <li><code>--num_frames</code>: Duration (frames = 8 √ó n + 1, so 121 = ~4 seconds at 30fps)</li>
                        <li><code>--seed</code>: For reproducibility</li>
                        <li><code>--pipeline_config</code>: Which model to use</li>
                    </ul>
                </div>

                <h3>Generate from an Image</h3>
                <p>Animate an existing image:</p>

                <div class="code-block">
<code>python inference.py \
  --prompt "The person walks forward with confidence, waving at the camera, bright sunny day" \
  --conditioning_media_paths path/to/image.jpg \
  --conditioning_start_frames 0 \
  --height 720 \
  --width 1280 \
  --num_frames 121 \
  --seed 42 \
  --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml</code>
                </div>

                <h3>Extend an Existing Video</h3>
                <p>Make your videos longer:</p>

                <div class="code-block">
<code>python inference.py \
  --prompt "The car continues driving through the desert as the sun sets" \
  --conditioning_media_paths path/to/video.mp4 \
  --conditioning_start_frames 120 \
  --height 720 \
  --width 1280 \
  --num_frames 241 \
  --seed 42 \
  --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml</code>
                </div>

                <div class="tip">
                    <strong>üìù Important:</strong> Input video frames must be (multiple of 8) + 1. Valid examples: 9, 17, 25, 33, 49, 57, 121, 257.
                </div>

                <h3>Using as a Python Library</h3>
                <p>Integrate LTX-Video into your Python applications:</p>

                <div class="code-block">
<code>from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt="Your prompt here",
        height=720,
        width=1280,
        num_frames=121,
        output_path="output.mp4",
        seed=42
    )
)</code>
                </div>
            </section>

            <!-- Usage Guide -->
            <section id="guide">
                <h2>üìö Complete Usage Guide</h2>

                <h3>1. Prompt Engineering for Better Results</h3>
                <p>
                    Good prompts are crucial for high-quality video generation. 
                    Here's how to write effective prompts:
                </p>

                <h4>Prompt Structure</h4>
                <ul>
                    <li><strong>Start with action:</strong> Begin with the main action in one sentence</li>
                    <li><strong>Add specific details:</strong> Describe movements, gestures, and how things change</li>
                    <li><strong>Describe appearances:</strong> Character and object descriptions</li>
                    <li><strong>Set the environment:</strong> Background, lighting, colors, atmosphere</li>
                    <li><strong>Specify camera work:</strong> Angles, movements, zoom, pans</li>
                    <li><strong>Keep it under 200 words:</strong> Focused prompts work better</li>
                </ul>

                <h4>Good Prompt Example</h4>
                <div class="highlight">
                    "A professional chef in a modern kitchen carefully plating a gourmet dish. The chef moves with precision, 
                    adding microgreens and sauce details with tweezers. Soft golden light from above illuminates the workspace. 
                    The camera slowly zooms in on the finished plate from above, showing the vibrant colors and composition."
                </div>

                <h4>Poor Prompt Example</h4>
                <div class="warning">
                    "A chef making food" - Too vague, lacks detail and specificity
                </div>

                <h3>2. Key Generation Parameters</h3>

                <h4>Guidance Scale</h4>
                <ul>
                    <li><strong>Recommended: 3.0 - 3.5</strong></li>
                    <li><strong>Lower values (1-2):</strong> More creative, less adherent to prompt</li>
                    <li><strong>Higher values (5+):</strong> Strict adherence but may reduce quality</li>
                </ul>

                <h4>Inference Steps</h4>
                <ul>
                    <li><strong>Fewer steps (8-20):</strong> Fast but lower quality</li>
                    <li><strong>Standard (20-30):</strong> Balanced quality and speed</li>
                    <li><strong>More steps (40+):</strong> Better quality but slower</li>
                </ul>

                <h4>Resolution Recommendations</h4>
                <ul>
                    <li><strong>1216√ó704 @ 30fps:</strong> Default, best balance (121 frames = ~4 sec)</li>
                    <li><strong>960√ó544 @ 24fps:</strong> Faster, good quality</li>
                    <li><strong>1280√ó720 @ 30fps:</strong> HD, higher quality</li>
                    <li><strong>1920√ó1080 @ 24fps:</strong> Full HD, slower</li>
                    <li><strong>3840√ó2160 @ 24fps:</strong> 4K, very slow, high VRAM</li>
                </ul>

                <h3>3. Multi-Keyframe Conditioning</h3>
                <p>Generate videos conditioned on multiple images/videos at different timestamps:</p>

                <div class="code-block">
<code>python inference.py \
  --prompt "A character walks from left to right across a landscape" \
  --conditioning_media_paths image1.jpg image2.jpg image3.jpg \
  --conditioning_start_frames 0 60 120 \
  --height 720 \
  --width 1280 \
  --num_frames 121 \
  --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml</code>
                </div>

                <h3>4. Control Models (IC-LoRA)</h3>
                <p>
                    Use specialized control models available in ComfyUI for precise control. 
                    Available controls include depth, pose, and Canny edge detection.
                </p>

                <div class="tip">
                    <strong>üí° Tip:</strong> Control models work through ComfyUI interface for now. 
                    Command-line support is being added.
                </div>

                <h3>5. Generation Quality Settings</h3>

                <h4>For Maximum Quality (Full Model)</h4>
                <div class="code-block">
<code>python inference.py \
  --prompt "Your high-detail prompt" \
  --height 1280 \
  --width 720 \
  --num_frames 121 \
  --guidance_scale 3.5 \
  --num_inference_steps 40 \
  --pipeline_config configs/ltxv-13b-0.9.8-dev.yaml</code>
                </div>

                <h4>For Fast Iteration (Distilled Model)</h4>
                <div class="code-block">
<code>python inference.py \
  --prompt "Your prompt" \
  --height 960 \
  --width 544 \
  --num_frames 121 \
  --num_inference_steps 8 \
  --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml</code>
                </div>

                <h3>6. Using Diffusers Integration</h3>
                <p>
                    If you prefer using the popular Diffusers library:
                </p>

                <div class="code-block">
<code>from diffusers import LTXVideoPipeline
import torch

pipe = LTXVideoPipeline.from_pretrained(
    "Lightricks/LTX-Video",
    torch_dtype=torch.float16,
    variant="fp16"
).to("cuda")

video_frames = pipe(
    prompt="Your prompt",
    height=720,
    width=1280,
    num_frames=121,
    guidance_scale=3.5,
    num_inference_steps=40
).frames

# Save video
from diffusers.utils import export_to_video
export_to_video(video_frames[0], "output.mp4")</code>
                </div>
            </section>

            <!-- Advanced Features -->
            <section id="advanced">
                <h2>üîß Advanced Features & Optimization</h2>

                <h3>1. Multi-Scale Rendering Pipeline</h3>
                <p>
                    Combine the full model and distilled model for optimal speed-quality trade-off:
                </p>
                <ul>
                    <li>Generate low-res with distilled model quickly</li>
                    <li>Upscale with spatial and temporal upscalers</li>
                    <li>Refine with full model for final polish</li>
                </ul>

                <h3>2. Video-to-Video Transformations</h3>
                <p>Modify existing videos using text prompts:</p>
                <ul>
                    <li>Change style or aesthetic</li>
                    <li>Apply effects or filters</li>
                    <li>Transform scenes while preserving motion</li>
                </ul>

                <h3>3. LoRA Fine-Tuning</h3>
                <p>
                    Train custom LoRA models for specific styles or effects using the 
                    <a href="https://github.com/Lightricks/LTX-Video-Trainer" target="_blank">LTX-Video-Trainer</a>:
                </p>
                <ul>
                    <li><strong>Style LoRAs:</strong> Custom art styles or aesthetics</li>
                    <li><strong>Control LoRAs:</strong> Custom control models (depth, pose, edges)</li>
                    <li><strong>Effect LoRAs:</strong> Specific visual effects</li>
                </ul>

                <div class="key-takeaway">
                    <strong>‚úÖ Memory Efficient:</strong> LoRA fine-tuning requires only 1GB VRAM compared to 24GB for full fine-tuning!
                </div>

                <h3>4. FP8 Quantization</h3>
                <p>Use 8-bit quantized models for faster inference with minimal quality loss:</p>
                <ul>
                    <li>Up to 3√ó speedup on compatible GPUs</li>
                    <li>Significant VRAM reduction</li>
                    <li>Minimal visual quality degradation</li>
                </ul>

                <h3>5. Stochastic Inference</h3>
                <p>Improve visual quality on distilled models by enabling stochastic sampling:</p>

                <div class="code-block">
<code>--use_stochastic_sampling True</code>
                </div>

                <h3>6. CPU Offloading</h3>
                <p>Move unused components to CPU to reduce VRAM usage:</p>

                <div class="code-block">
<code>--offload_unused_components True</code>
                </div>

                <h3>7. ComfyUI Integration</h3>
                <p>
                    For a visual interface, use the official ComfyUI integration at 
                    <a href="https://github.com/Lightricks/ComfyUI-LTXVideo" target="_blank">ComfyUI-LTXVideo</a>:
                </p>
                <ul>
                    <li>Visual node-based workflow</li>
                    <li>Easy parameter adjustment</li>
                    <li>Support for control models</li>
                    <li>Batch processing capabilities</li>
                </ul>

                <h3>8. Community Tools</h3>
                <p>The community has created additional tools for LTX-Video:</p>
                <ul>
                    <li><strong>ComfyUI-LTXTricks:</strong> Advanced sampling techniques (RF-Inversion, RF-Edit, FlowEdit)</li>
                    <li><strong>LTX-VideoQ8:</strong> Optimized 8-bit version for consumer GPUs</li>
                    <li><strong>TeaCache for LTX-Video:</strong> 2√ó speedup without retraining</li>
                </ul>
            </section>

            <!-- Tips & Optimization -->
            <section id="tips">
                <h2>üí° Pro Tips & Optimization</h2>

                <h3>Quality Improvement Tips</h3>
                <ul>
                    <li><strong>Detailed prompts:</strong> More specific descriptions yield better results</li>
                    <li><strong>Reference images:</strong> Use image conditioning for consistency</li>
                    <li><strong>Multiple keyframes:</strong> Guide generation at different points</li>
                    <li><strong>Iterative refinement:</strong> Generate multiple times and pick the best</li>
                    <li><strong>Seed variation:</strong> Try different seeds with the same prompt</li>
                </ul>

                <h3>Performance Optimization Tips</h3>
                <ul>
                    <li><strong>Choose right model:</strong> Distilled for speed, full for quality</li>
                    <li><strong>Lower resolution first:</strong> Preview at lower res, then upscale</li>
                    <li><strong>Fewer steps for preview:</strong> Use 8 steps for fast preview, 30+ for final</li>
                    <li><strong>Batch processing:</strong> Process multiple videos to maximize GPU utilization</li>
                    <li><strong>Use FP8:</strong> 3√ó speedup on compatible GPUs</li>
                </ul>

                <h3>Memory Management</h3>
                <ul>
                    <li><strong>Enable offloading:</strong> Move components to CPU when not in use</li>
                    <li><strong>Monitor VRAM:</strong> Use <code>nvidia-smi</code> to track usage</li>
                    <li><strong>Clear cache:</strong> Use <code>torch.cuda.empty_cache()</code> between generations</li>
                    <li><strong>Reduce batch size:</strong> Generate fewer frames at once</li>
                </ul>

                <h3>VRAM Requirements by Model</h3>
                <ul>
                    <li><strong>RTX 4090 (24GB):</strong> Can run full 13B model comfortably</li>
                    <li><strong>RTX 4080 (16GB):</strong> Use distilled models or FP8</li>
                    <li><strong>RTX 4070 (12GB):</strong> Use 2B-distilled or 13B-distilled-fp8</li>
                    <li><strong>RTX 4060 (8GB):</strong> Use 2B-distilled-fp8 or limit resolution</li>
                </ul>

                <div class="warning">
                    <strong>‚ö†Ô∏è Out of Memory?</strong> Try these steps:
                    <ul>
                        <li>Reduce resolution (divisible by 32)</li>
                        <li>Use distilled model instead</li>
                        <li>Enable FP8 quantization</li>
                        <li>Enable CPU offloading</li>
                        <li>Reduce number of frames</li>
                    </ul>
                </div>

                <h3>Best Practices for Production</h3>
                <ul>
                    <li><strong>Use fixed seeds:</strong> For reproducibility</li>
                    <li><strong>Log parameters:</strong> Track what settings produced good results</li>
                    <li><strong>Version control:</strong> Save prompts and configs in git</li>
                    <li><strong>A/B testing:</strong> Compare different models and parameters</li>
                    <li><strong>Quality assurance:</strong> Preview at full resolution before final render</li>
                </ul>
            </section>

            <!-- Resources -->
            <section id="resources">
                <h2>üìö Resources & Further Learning</h2>

                <div class="resources">
                    <h3>Official Resources</h3>
                    <ul>
                        <li>
                            <a href="https://github.com/Lightricks/LTX-Video" target="_blank">üìå LTX-Video GitHub Repository</a>
                            - Main project repository
                        </li>
                        <li>
                            <a href="https://arxiv.org/abs/2501.00103" target="_blank">üìÑ Research Paper</a>
                            - "LTX-Video: Realtime Video Latent Diffusion"
                        </li>
                        <li>
                            <a href="https://docs.ltx.video" target="_blank">üìñ Official Documentation</a>
                            - Complete reference documentation
                        </li>
                        <li>
                            <a href="https://huggingface.co/Lightricks/LTX-Video" target="_blank">ü§ó Hugging Face Model Hub</a>
                            - Download models and weights
                        </li>
                    </ul>

                    <h3>Integrations & Tools</h3>
                    <ul>
                        <li>
                            <a href="https://github.com/Lightricks/ComfyUI-LTXVideo" target="_blank">üé® ComfyUI Integration</a>
                            - Visual node-based interface
                        </li>
                        <li>
                            <a href="https://github.com/Lightricks/LTX-Video-Trainer" target="_blank">üõ†Ô∏è LTX-Video-Trainer</a>
                            - LoRA fine-tuning repository
                        </li>
                        <li>
                            <a href="https://app.ltx.studio" target="_blank">üåê LTX-Studio</a>
                            - Online web interface for generation
                        </li>
                    </ul>

                    <h3>Community Projects</h3>
                    <ul>
                        <li>
                            <a href="https://github.com/logtd/ComfyUI-LTXTricks" target="_blank">‚ú® ComfyUI-LTXTricks</a>
                            - Advanced sampling techniques (RF-Inversion, RF-Edit, FlowEdit)
                        </li>
                        <li>
                            <a href="https://github.com/KONAKONA666/LTX-Video" target="_blank">üé± LTX-VideoQ8</a>
                            - 8-bit optimized version for consumer GPUs
                        </li>
                        <li>
                            <a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video" target="_blank">üçµ TeaCache</a>
                            - 2√ó speedup without retraining
                        </li>
                    </ul>

                    <h3>Online Demos</h3>
                    <ul>
                        <li>
                            <a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b" target="_blank">üé¨ LTX-Studio (13B-mix)</a>
                        </li>
                        <li>
                            <a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv" target="_blank">üé¨ LTX-Studio (13B-distilled)</a>
                        </li>
                        <li>
                            <a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video" target="_blank">üåê Fal.ai (13B full)</a>
                        </li>
                        <li>
                            <a href="https://replicate.com/lightricks/ltx-video" target="_blank">üåê Replicate</a>
                        </li>
                    </ul>

                    <h3>Community & Support</h3>
                    <ul>
                        <li>
                            <a href="https://discord.gg/ltxplatform" target="_blank">üí¨ Official Discord</a>
                            - Community chat and support
                        </li>
                        <li>
                            <a href="https://github.com/Lightricks/LTX-Video/issues" target="_blank">üêõ GitHub Issues</a>
                            - Bug reports and feature requests
                        </li>
                    </ul>

                    <h3>Related Technologies to Learn</h3>
                    <ul>
                        <li>Diffusion Models and Transformers</li>
                        <li>DiT (Diffusion Transformers) Architecture</li>
                        <li>Latent Space Representations</li>
                        <li>Video Compression and Upscaling</li>
                        <li>Fine-tuning with LoRA</li>
                        <li>ComfyUI Workflow Design</li>
                    </ul>
                </div>
            </section>

            <!-- Conclusion -->
            <section>
                <h2>üéâ Wrapping Up</h2>
                <p>
                    You now have everything you need to start creating amazing AI-generated videos with LTX-Video! 
                    This is an exciting time for video generation, and LTX-Video represents a major step forward in 
                    making professional-quality video generation accessible to everyone.
                </p>

                <h3>Next Steps</h3>
                <ol>
                    <li>Install LTX-Video following the setup guide above</li>
                    <li>Generate your first video with a text prompt</li>
                    <li>Experiment with different prompts and parameters</li>
                    <li>Explore image-to-video and multi-keyframe features</li>
                    <li>Join the community and share your creations</li>
                    <li>Consider fine-tuning models for your specific use case</li>
                </ol>

                <div class="highlight">
                    <strong>üåü Remember:</strong> The best way to learn is by experimenting. 
                    Start with simple prompts, gradually increase complexity, and don't be afraid to try new things!
                </div>

                <h3>Key Takeaways</h3>
                <ul>
                    <li>‚úÖ LTX-Video is a powerful, open-source video generation model</li>
                    <li>‚úÖ Multiple model sizes for different hardware and quality needs</li>
                    <li>‚úÖ Supports text-to-video, image-to-video, and advanced conditioning</li>
                    <li>‚úÖ Can run locally on consumer GPUs</li>
                    <li>‚úÖ Active community with many tools and integrations</li>
                    <li>‚úÖ Continuous updates and improvements from Lightricks</li>
                </ul>

                <div class="success">
                    <strong>üöÄ Ready to Create?</strong> Start generating videos now! Visit 
                    <a href="https://github.com/Lightricks/LTX-Video" target="_blank">the GitHub repository</a> 
                    and begin your AI video creation journey.
                </div>
            </section>
        </main>

        <footer>
            <p>
                <strong>Educational Tutorial:</strong> This guide is based on the 
                <a href="https://github.com/Lightricks/LTX-Video" target="_blank">LTX-Video Official Repository</a> 
                by Lightricks
            </p>
            <p style="margin-top: 15px;">
                Licensed under <a href="https://github.com/Lightricks/LTX-Video/blob/main/LICENSE" target="_blank">Apache 2.0</a>
            </p>
            <p style="margin-top: 15px; font-size: 0.9em; color: #999;">
                For the latest updates and features, please visit the official repository and documentation.
                <br>
                Last updated: February 2026 | LTX-Video Version 0.9.8+
            </p>
        </footer>
    </div>
</body>
</html>